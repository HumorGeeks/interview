

# CPU 三级缓存架构

## CPU 读取存储器数据过程

1. CPU要取寄存器X的值，只需要一步：直接读取
2. PU要取L1 cache的某个值，需要1-3步（或者更多）：把cache行锁住，把某个数据拿来，解锁，如果没锁住就慢了
3. CPU要取L2 cache的某个值，先要到L1 cache里取，L1当中不存在，在L2里，L2开始加锁，加锁以后，把L2里的数据复制到L1，再执行读L1的过程，上面的3步，再解锁
4. CPU取L3 cache的也是一样，只不过先由L3复制到L2，从L2复制到L1，从L1到CPU
5. CPU取内存则最复杂：通知内存控制器占用总线带宽，通知内存加锁，发起内存读请求，等待回应，回应数据保存到L3（如果没有就到L2），再从L3/2到L1，再从L1到CPU，之后解除总线锁定

## 时间局部性

> 如果一个信息项正在被访问，那么在近期它很可能还会被再次访问。比如循环、递归、方法的反复调用等

## 空间局部性

> 如果一个存储器的位置被引用，那么将来他附近的位置也会被引用。比如顺序执行的代码、连续创建的两个对象、数组等

## 带有高速缓存的CPU执行计算的流程

1. 程序以及数据被加载到主内存
2. 指令和数据被加载到CPU的高速缓存
3. CPU执行指令，把结果写到高速缓存
4. 高速缓存中的数据写回主内存

## CPU运行安全等级

ring0（内核态）：可以运行所有操作；操作系统内部内部程序指令通常运行在ring0级别（修改内存；mysql 数据（undo和redo）刷盘；）

ring1

ring2

ring3：（用户态）操作系统以外的第三方程序运行在ring3级别第三方程序如果要调用操作,系统内部函数功能，由于运行安全级别不够,必须切换CPU运行状态，从ring3切换到ring0,然后执行系统函数，JVM创建线程，线程阻塞唤醒是重型操作了，因为CPU要切换运行状态

PS：进程和线程对应有两个堆栈；一个在用户空间里边；一个在内核空间里边

## 线程模型

> CPU调度的基本单位是线程，也划分为

用户线程模型：线程创建、销毁、调度由用户程序实现

线程阻塞不会引起进程阻塞。在多处理器系统上，多线程在多处理器上并行运行。线程的创建、调度和管
理由内核完成，效率比ULT要慢，比进程操作快

内核线程模型：线程创建、销毁、调度由操作系统完成

## 进程与线程

进程：现代操作系统在运行一个程序时，会为其创建一个进程；例如，启动一个Java程序，操作系统就会创建一个Java进程。进程是OS(操作系统)资源分配的最小单位

线程：线程是OS(操作系统)调度CPU的最小单元，也叫轻量级进程（Light Weight Process），在一个进程里可以创建多个线程，这些线程都拥有各自的计数器、堆栈和局部变量等属性，并且能够访问共享的内存变量

## 线程上下文切换

> CPU切换前把当前任务的状态保存下来，以便下次切换回这个任务时可以再次加载这个任务的状态，然后加载下一任务的状态并执行。任务的状态保存及再加载, 这段过程就叫做上下文切换

## 线程上下文切换问题

- **直接消耗**：指的是CPU寄存器需要保存和加载, 系统调度器的代码需要执行, TLB实例需要重新加载, CPU 的pipeline需要刷掉
- **间接消耗**：指的是多核的cache之间得共享数据, 间接消耗对于程序的影响要看线程工作区操作数据的大小

## 虚拟机指令架构

1. 栈指令集架构
   1. 设计和实现更简单,适用于资源受限的系统;
   2. 避开了寄存器的分配难题:使用零地址指令方式分配;
   3. 指令流中的指令大部分是零地址指令,其执行过程依赖与操作栈,指令集更小,编译器容易实现;
   4. 不需要硬件支持,可移植性更好,更好实现跨平台
2. 寄存器指令集架构
   1. 典型的应用是x86的二进制指令集:比如传统的PC以及Android的Davlik虚拟机。
   2. 指令集架构则完全依赖硬件,可移植性差。
   3. 性能优秀和执行更高效。
   4. 花费更少的指令去完成一项操作。
   5. 在大部分情况下,基于寄存器架构的指令集往往都以一地址指令、二地址指令和三地址指令为主,而基于栈式架构的指令集却是以零地址指令为主。Java符合典型的栈指令集架构特征，像Python、Go都属于这种架构。

# JMM

主内存

主要存储的是Java实例对象，所有线程创建的实例对象都存放在主内存中，不管该实例对象是成员变量还是方法中的本地变量(也称局部变量)，当然也包括了共享的类信息、常量、静态变量。由于是共享数据区域，多条线程对同一个变量进行访问可能会发生线程安全问题

工作内存

存储当前方法的所有本地变量信息(工作内存中存储着主内存中的变量副本拷贝)，每个线程只能访问自己的工作内存，即线程中的本地变量对其它线程是不可见的，就算是两个线程执行的是同一段代码，它们也会各自在自己的工作内存中创建属于当前线程的本地变量，当然也包括了字节码行号指示器、相关Native方法的信息

## 数据同步八大原子操作

1. lock(锁定)：作用于主内存的变量，把一个变量标记为一条线程独占状态
2. unlock(解锁)：作用于主内存的变量，把一个处于锁定状态的变量释放出来，释放后的变量才可以被其他线程锁定
3. read(读取)：作用于主内存的变量，把一个变量值从主内存传输到线程的工作内存中，以便随后的load动作使用
4. load(载入)：作用于工作内存的变量，它把read操作从主内存中得到的变量值放入工作内存的变量副本中
5. use(使用)：作用于工作内存的变量，把工作内存中的一个变量值传递给执行引擎
6. assign(赋值)：作用于工作内存的变量，它把一个从执行引擎接收到的值赋给工作内存的变量
7. store(存储)：作用于工作内存的变量，把工作内存中的一个变量的值传送到主内存中，以便随后的write的操作
8. write(写入)：作用于工作内存的变量，它把store操作从工作内存中的一个变量的值传送到主内存的变量中

## 并发编程特性

1. 可见性（volatile）不保证原子性

   - 代码层面：共享变量改变后其他线程**立刻**感知到修改
   - 字节码层面：多一个访问标志ACC_VOLATILE
   - 缓存一致性协议

2. 原子性：原子性指的是一个操作是不可中断的，即使是在多线程环境下，一个操作一旦开始就不会被其他线程影响（synchronized和Lock）

3. 有序性：

   > 在Java里面，可以通过volatile关键字来保证一定的“有序性”（具体原理在下一节讲述volatile关键字）。另外可以通过synchronized和Lock来保证有序性，很显然，synchronized和Lock保证每个时刻是有一个线程执行同步代码，相当于是让线程顺序执行同步代码，自然就保证了有序性

## Java内存模型

> 每个线程都有自己的工作内存（类似于前面的高速缓存）。线程对变量的所有操作都必须在工作内存中进行，而不能直接对主存进行操作。并且每个线程不能访问其他线程的工作内存

## 指令重排序

> 只要程序的最终结果与它顺序化情况的结果相等，那么指令的执行顺序可以与代码顺序不一致，此过程叫指令的重排序

## 内存屏障

1. LoadLoad屏障：对于这样的语句Load1; LoadLoad; Load2，在Load2及后续读取操作要读取的数据被访问前，保证Load1要读取的数据被读取完毕。
2. StoreStore屏障：对于这样的语句Store1; StoreStore; Store2，在Store2及后续写入操作执行前，保证Store1的写入操作对其它处理器可见。
3. LoadStore屏障：对于这样的语句Load1; LoadStore; Store2，在Store2及后续写入操作被刷出前，保证Load1要读取的数据被读取完毕。
4. StoreLoad屏障：对于这样的语句Store1; StoreLoad; Load2，在Load2及后续所有读取操作执行前，保证Store1的写入对所有处理器可见。它的开销是四种屏障中最大的。在大多数处理器的实现中，这个屏障是个万能屏障，兼具其它三种内存屏障的功能

### 硬件层

## volatile禁止重排优化

> 禁止指令重排优化，从而避免多线程环境下程序出现乱序执行的现象

## volatile重排序规则表

| 是否能重排序 | 第二个操作 |            |            |
| ------------ | ---------- | ---------- | ---------- |
| 第一个操作   | 普通读写   | volatile读 | volatile写 |
| 普通读写     |            |            | NO         |
| volatile读   | NO         | NO         | NO         |
| volatile写   |            | NO         | NO         |

PS：

1. 当第二个操作是volatile 写时，不管第一个操作是什么，都不能重排序。这个规则确保volatile 写之前的操作不会被编译器重排序到volatile 写之后。
2. 当第一个操作是volatile 读时，不管第二个操作是什么，都不能重排序。这个规则确保volatile 读之后的操作不会被编译器重排序到volatile 读之前。
3. 当第一个操作是volatile 写，第二个操作是volatile 读时，不能重排序。

## JVM-JMM-CPU底层全执行流程

![](https://gitee.com/HumorGeeks/img/raw/master/img/202112091039052.jpg)

## 目前操作系统主要结构

![](https://gitee.com/HumorGeeks/img/raw/master/img/202112091422636.png)

## 硬件缓存锁定机制

### 总线锁

> 前端总线(也叫CPU总线)是所有CPU与芯片组连接的主干道，负责CPU与外界所有部件的通信，包括高速缓存、内存、北桥，其控制总线向各个部件发送控制信号、通过地址总线发送地址信号指定其要访问的部件、通过数据总线双向传输

在CPU1要做 i++操作的时候，其在总线上发出一个LOCK#信号，其他处理器就不能操作缓存了该共享变量内存地址的缓存，也就是阻塞了其他CPU，使该处理器可以独享此共享内存

### MESI协议

> 多核CPU的情况下有多个一级缓存，如何保证缓存内部数据的一致,不让系统数据混乱。这里就引出了一个一致性的协议MESI

PS：

**缓存一致性协议基于缓存行，如果超过大小则会升级为总线锁；总线裁决效率远高于总线锁**

**volatile 无法保证原子性**

**缓存一致性协议只能对缓存行有效，二对于寄存器无效**

# 如何解决线程并发安全问题

> 序列化访问临界资源。即在同一时刻，只能有一个线程访问临界资源，也称作同步互斥访问；锁的优化、膨胀过程，锁的状态记录在哪，标志位，消除，逃逸分析，（为什么要有，怎么去做）

面试怎么说：

synchronized是一种互斥锁，怎么去用（方法粒度）；原理层面（字节码如何判断）

## 实现方式

### synchronized

> synchronized内置锁是一种对象锁(锁的是对象而非引用)，作用粒度是对象，可以用来实现对临界资源的同步互斥访问，是可
> 重入的；加锁解锁由JVM完成

#### 内部实现

![](https://gitee.com/HumorGeeks/img/raw/master/img/202112101007897.png)

Pthread：系统维护

#### 加锁方式

1. 同步实例方法，锁是当前实例对象
2. 同步类方法，锁是当前类对象
3. 同步代码块，锁是括号里面的对象

PS：sout 为同步方法，多个线程同时调用会影响性能

#### 对象的内存布局

![](https://gitee.com/HumorGeeks/img/raw/master/img/202112091630799.png)

#### 对象头如何记录对象锁

![](https://gitee.com/HumorGeeks/img/raw/master/img/202112091632229.png)

PS：**现在我们虚拟机基本是64位的，而64位的对象头有点浪费空间,JVM默认会开启指针压缩，所以基本上也是按32位的形式记录对象头**
**的**

#### 偏向锁、轻量级锁、重量级锁对应的hashcode放在哪里

偏向锁：调用hashcode后，system.lazyhashcode；会升级成轻量级锁

轻量级锁：lock record（markword）

当解释器执行monitorenter字节码轻度锁住一个对象时，就会在获取锁的线程的栈上显式或者隐式分配一个LockRecord，这个LockRecord存储锁对象markword的拷贝(Displaced Mark Word)

重量级锁：记录在monitor里边

PS：jol-core 用于打印对象内存数据

PS：操作系统的大端和小端模式

PS：IVM 内部将偏向锁延迟4s（默认）启动，JVM启动依赖大量的hashmap，class类，大量同步块，十几个线程；避免无谓的偏向锁-轻量级锁-重量级的升级过程

PS：无锁状态（匿名偏向，可偏向状态，只是没有指定线程ID）

PS：偏向锁调用hashcode后，system.lazyhashcode；会升级成轻量级锁（因为内部没有记录hashcode）

![](https://gitee.com/HumorGeeks/img/raw/master/img/202112091659250.png)

#### 线程逃逸分析

#### 优化

1. 锁的膨胀升级（不可逆）
2. 锁的粗化
3. 锁的消除
4. 自旋锁

### AQS 特性

1. 公平/非公平
2. 可重入
3. 允许中断
4. 共享/独占
5. 阻塞等待队列

### 线程阻塞和唤醒方式

1. suspend()、resume()

   > 线程被挂起以后不会释放锁，可能与其他线程、主线程产生死锁

2. wait、notify

   > 1、对象操作都需要加同步synchronized；
   >
   > 2、线程需要阻塞的地方调用对象的wait方法；
   >
   > 存在的不足：面向对象的阻塞是阻塞当前线程，而唤醒的是随机的一个线程或者所有线程，偏重线程间的通信；同时某一线程在被另一线程notify之前必须要保证此线程已经执行到wait等待点，错过notify则可能永远都在等待

3. LockSupport提供的park和unpark方法

   > park与unpark方法控制的颗粒度更加细小，能准确决定线程在某个点停止，进而避免死锁的产生

### CAS

> load之前，都会有一个except值；如果被其他线程修改主内存值已经发生改变，比较except和主内存中的值，不相等重新load然后进行修改----------原子操作

![](https://gitee.com/HumorGeeks/img/raw/master/img/202112101507102.png)

#### 实现（Unsafe类）

> ```java
> public final native boolean compareAndSwapObject(Object var1, long var2, Object var4, Object var5);
> 
> public final native boolean compareAndSwapInt(Object var1, long var2, int var4, int var5);
> 
> public final native boolean compareAndSwapLong(Object var1, long var2, long var4, long var6);
> ```

### ReentrantLock

ReentrantLock是一种基于AQS框架的应用实现，是JDK中的一种线程并发访问的同步手段，它的功能类似于synchronized是一种互斥锁，可以保证线程安全。而且它具有比synchronized更多的特性，比如它支持手动加锁与解锁，支持加锁的公平性

> ReentrantLock如何实现synchronized不具备的公平与非公平性呢？
> 在ReentrantLock内部定义了一个Sync的内部类，该类继承AbstractQueuedSynchronized，对该抽象类的部分方法做了实现；并且还定义了两个子类：
> 1、FairSync 公平锁的实现
> 2、NonfairSync 非公平锁的实现
> 这两个类都继承自Sync，也就是间接继承了AbstractQueuedSynchronized，所以这一个ReentrantLock同时具备公平与非公平特性。上面主要涉及的设计模式：模板模式-子类根据需要做具体业务实现

PS：除了Lock外，Java.concurrent.util当中同步器的实现如Latch,Barrier,BlockingQueue等，都是基于AQS框架实现

PS：如何查看类和所有子类



Lock，公平与公平两种特性

三大核心原理

自旋，LocksSuport, CAS，queue队列

CAS依赖汇编指令：cmpxchg()

Lock可重入性：可重入！

synchronized：可重入

> 内部原理后期深入了解下（并发编程到时候好好看一下精髓，自己总结）

~~~java
exclusiveOwnerThread 当前获取锁的线程是谁！
state 状态器

public final void acquire(int arg) {
    if (!tryAcquire(arg) &&
        acquireQueued(addWaiter(Node.EXCLUSIVE), arg))
        selfInterrupt();
}

tryAcquire(arg) //锁竞争逻辑
//CLH 
addWaiter(Node.EXCLUSIVE) //线程入队,Node节点，Node对Thread引用
    Node：共享属性，独占属性 //响应式编程，异步非阻塞，FutureTask，Callbale
    创建节点Node = pre,next,waitestate,thread 重要属性
    waitestate节点的生命状态：信号量
        SIGNAL = -1 //可被唤醒
        CANCELLED = 1 //代表出现异常，中断引起的，需要废弃结束
        CONDITION = -2 // 条件等待
        PROPAGATE = -3 // 传播
        0 - 初始状态Init状态
为了保证所有阻塞线程对象能够被唤醒
compareAndSetTail(t, node) 入队也存在竞争
    
//当前节点,线程要开始阻塞
acquireQueued(Node(currentThread), arg)
    节点阻塞之前还得再尝试一次获取锁：
    1，能够获取到，节点出队，并且把head往后挪一个节点，新的头结点就是当前节点
    2、不能获取到，阻塞等待被唤醒
    	1.首先第1轮循环、修改head的状态，修改成sinal=-1标记处可以被唤醒.
    	2.第2轮循环，阻塞线程，并且需要判断线程是否是有中断信号唤醒的！
    	shouldParkAfterFailedAcquire(p, node)
    waitestate = 0 - > -1 head节点为什么改到-1，因为持有锁的线程T0在释放锁的时候，得判断head节点的waitestate是否!=0,如果！=0成立，会再把waitstate = -1->0,要想唤醒排队的第一个线程T1，T1被唤醒再接着走循环，去抢锁，可能会再失败（在非公平锁场景下），此时可能有线程T3持有了锁！T1可能再次被阻塞，head的节点状态需要再一次经历两轮循环：waitState = 0 -> -1
   Park阻塞线程唤醒有两种方式：
    1、中断
    2、release()
~~~

> 用多角度，多个线程不断去切换的一种解决问题的思维，抽象思维能力

### Lock（生产上如何实现也是这个思路）

> 显式锁

1. **自旋**
2. **CAS**
3. **LockSupport**
4. queue队列

# BlockingQueue

> 线程通信工具，不管并发多高，同一时间只有一个线程能对对队列进行入队和出队操作

1. 线程安全

## 应用场景

1. 线程池
2. Eureka的三级缓存
3. springcloud
4. Nacos
5. Netty
6. Rocketmq

## ArrayBlockingQueue源码剖析

~~~java
/*初始化一个ArrayBlockingQueue*/    
    public ArrayBlockingQueue(int capacity, boolean fair) {
            if (capacity <= 0)
                throw new IllegalArgumentException();
            this.items = new Object[capacity];
            lock = new ReentrantLock(fair);
            // 未放满状态量
            notEmpty = lock.newCondition();
            // 未空状态量
            notFull =  lock.newCondition();
        }

    public void put(E e) throws InterruptedException {
        checkNotNull(e);
        final ReentrantLock lock = this.lock;//加锁
        lock.lockInterruptibly();
        try {
            while (count == items.length)
                notFull.await();
            enqueue(e);
        } finally {
            lock.unlock();
        }
    }
~~~

线程通信工具

1. BlockingQueue
2. Semaphore
3. CountDownLatch
4. CyclicBarrier

# Semaphore

> 它的作用是控制访问特定资源的线程数目，底层依赖AQS的状态State，是在生产当中比较常用的一个工具类









