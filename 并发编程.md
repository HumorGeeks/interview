

# CPU 三级缓存架构

## CPU 读取存储器数据过程

1. CPU要取寄存器X的值，只需要一步：直接读取
2. PU要取L1 cache的某个值，需要1-3步（或者更多）：把cache行锁住，把某个数据拿来，解锁，如果没锁住就慢了
3. CPU要取L2 cache的某个值，先要到L1 cache里取，L1当中不存在，在L2里，L2开始加锁，加锁以后，把L2里的数据复制到L1，再执行读L1的过程，上面的3步，再解锁
4. CPU取L3 cache的也是一样，只不过先由L3复制到L2，从L2复制到L1，从L1到CPU
5. CPU取内存则最复杂：通知内存控制器占用总线带宽，通知内存加锁，发起内存读请求，等待回应，回应数据保存到L3（如果没有就到L2），再从L3/2到L1，再从L1到CPU，之后解除总线锁定

## 时间局部性

> 如果一个信息项正在被访问，那么在近期它很可能还会被再次访问。比如循环、递归、方法的反复调用等

## 空间局部性

> 如果一个存储器的位置被引用，那么将来他附近的位置也会被引用。比如顺序执行的代码、连续创建的两个对象、数组等

## 带有高速缓存的CPU执行计算的流程

1. 程序以及数据被加载到主内存
2. 指令和数据被加载到CPU的高速缓存
3. CPU执行指令，把结果写到高速缓存
4. 高速缓存中的数据写回主内存

## CPU运行安全等级

ring0（内核态）：可以运行所有操作；操作系统内部内部程序指令通常运行在ring0级别（修改内存；mysql 数据（undo和redo）刷盘；）

ring1

ring2

ring3：（用户态）操作系统以外的第三方程序运行在ring3级别第三方程序如果要调用操作,系统内部函数功能，由于运行安全级别不够,必须切换CPU运行状态，从ring3切换到ring0,然后执行系统函数，JVM创建线程，线程阻塞唤醒是重型操作了，因为CPU要切换运行状态

PS：进程和线程对应有两个堆栈；一个在用户空间里边；一个在内核空间里边

## 线程模型

> CPU调度的基本单位是线程，也划分为

用户线程模型：线程创建、销毁、调度由用户程序实现

线程阻塞不会引起进程阻塞。在多处理器系统上，多线程在多处理器上并行运行。线程的创建、调度和管
理由内核完成，效率比ULT要慢，比进程操作快

内核线程模型：线程创建、销毁、调度由操作系统完成

## 进程与线程

进程：现代操作系统在运行一个程序时，会为其创建一个进程；例如，启动一个Java程序，操作系统就会创建一个Java进程。进程是OS(操作系统)资源分配的最小单位

线程：线程是OS(操作系统)调度CPU的最小单元，也叫轻量级进程（Light Weight Process），在一个进程里可以创建多个线程，这些线程都拥有各自的计数器、堆栈和局部变量等属性，并且能够访问共享的内存变量

## 线程上下文切换

> CPU切换前把当前任务的状态保存下来，以便下次切换回这个任务时可以再次加载这个任务的状态，然后加载下一任务的状态并执行。任务的状态保存及再加载, 这段过程就叫做上下文切换

## 线程上下文切换问题

- **直接消耗**：指的是CPU寄存器需要保存和加载, 系统调度器的代码需要执行, TLB实例需要重新加载, CPU 的pipeline需要刷掉
- **间接消耗**：指的是多核的cache之间得共享数据, 间接消耗对于程序的影响要看线程工作区操作数据的大小

## 虚拟机指令架构

1. 栈指令集架构
   1. 设计和实现更简单,适用于资源受限的系统;
   2. 避开了寄存器的分配难题:使用零地址指令方式分配;
   3. 指令流中的指令大部分是零地址指令,其执行过程依赖与操作栈,指令集更小,编译器容易实现;
   4. 不需要硬件支持,可移植性更好,更好实现跨平台
2. 寄存器指令集架构
   1. 典型的应用是x86的二进制指令集:比如传统的PC以及Android的Davlik虚拟机。
   2. 指令集架构则完全依赖硬件,可移植性差。
   3. 性能优秀和执行更高效。
   4. 花费更少的指令去完成一项操作。
   5. 在大部分情况下,基于寄存器架构的指令集往往都以一地址指令、二地址指令和三地址指令为主,而基于栈式架构的指令集却是以零地址指令为主。Java符合典型的栈指令集架构特征，像Python、Go都属于这种架构。

# JMM

主内存

主要存储的是Java实例对象，所有线程创建的实例对象都存放在主内存中，不管该实例对象是成员变量还是方法中的本地变量(也称局部变量)，当然也包括了共享的类信息、常量、静态变量。由于是共享数据区域，多条线程对同一个变量进行访问可能会发生线程安全问题

工作内存

存储当前方法的所有本地变量信息(工作内存中存储着主内存中的变量副本拷贝)，每个线程只能访问自己的工作内存，即线程中的本地变量对其它线程是不可见的，就算是两个线程执行的是同一段代码，它们也会各自在自己的工作内存中创建属于当前线程的本地变量，当然也包括了字节码行号指示器、相关Native方法的信息

## 数据同步八大原子操作

1. lock(锁定)：作用于主内存的变量，把一个变量标记为一条线程独占状态
2. unlock(解锁)：作用于主内存的变量，把一个处于锁定状态的变量释放出来，释放后的变量才可以被其他线程锁定
3. read(读取)：作用于主内存的变量，把一个变量值从主内存传输到线程的工作内存中，以便随后的load动作使用
4. load(载入)：作用于工作内存的变量，它把read操作从主内存中得到的变量值放入工作内存的变量副本中
5. use(使用)：作用于工作内存的变量，把工作内存中的一个变量值传递给执行引擎
6. assign(赋值)：作用于工作内存的变量，它把一个从执行引擎接收到的值赋给工作内存的变量
7. store(存储)：作用于工作内存的变量，把工作内存中的一个变量的值传送到主内存中，以便随后的write的操作
8. write(写入)：作用于工作内存的变量，它把store操作从工作内存中的一个变量的值传送到主内存的变量中

## 并发编程特性

1. 可见性（volatile）不保证原子性

   - 代码层面：共享变量改变后其他线程**立刻**感知到修改
   - 字节码层面：多一个访问标志ACC_VOLATILE
   - 缓存一致性协议

2. 原子性：原子性指的是一个操作是不可中断的，即使是在多线程环境下，一个操作一旦开始就不会被其他线程影响（synchronized和Lock）

3. 有序性：

   > 在Java里面，可以通过volatile关键字来保证一定的“有序性”（具体原理在下一节讲述volatile关键字）。另外可以通过synchronized和Lock来保证有序性，很显然，synchronized和Lock保证每个时刻是有一个线程执行同步代码，相当于是让线程顺序执行同步代码，自然就保证了有序性

## Java内存模型

> 每个线程都有自己的工作内存（类似于前面的高速缓存）。线程对变量的所有操作都必须在工作内存中进行，而不能直接对主存进行操作。并且每个线程不能访问其他线程的工作内存

## 指令重排序

> 只要程序的最终结果与它顺序化情况的结果相等，那么指令的执行顺序可以与代码顺序不一致，此过程叫指令的重排序

## 内存屏障

1. LoadLoad屏障：对于这样的语句Load1; LoadLoad; Load2，在Load2及后续读取操作要读取的数据被访问前，保证Load1要读取的数据被读取完毕。
2. StoreStore屏障：对于这样的语句Store1; StoreStore; Store2，在Store2及后续写入操作执行前，保证Store1的写入操作对其它处理器可见。
3. LoadStore屏障：对于这样的语句Load1; LoadStore; Store2，在Store2及后续写入操作被刷出前，保证Load1要读取的数据被读取完毕。
4. StoreLoad屏障：对于这样的语句Store1; StoreLoad; Load2，在Load2及后续所有读取操作执行前，保证Store1的写入对所有处理器可见。它的开销是四种屏障中最大的。在大多数处理器的实现中，这个屏障是个万能屏障，兼具其它三种内存屏障的功能

### 硬件层

## volatile禁止重排优化

> 禁止指令重排优化，从而避免多线程环境下程序出现乱序执行的现象

## volatile重排序规则表

| 是否能重排序 | 第二个操作 |            |            |
| ------------ | ---------- | ---------- | ---------- |
| 第一个操作   | 普通读写   | volatile读 | volatile写 |
| 普通读写     |            |            | NO         |
| volatile读   | NO         | NO         | NO         |
| volatile写   |            | NO         | NO         |

PS：

1. 当第二个操作是volatile 写时，不管第一个操作是什么，都不能重排序。这个规则确保volatile 写之前的操作不会被编译器重排序到volatile 写之后。
2. 当第一个操作是volatile 读时，不管第二个操作是什么，都不能重排序。这个规则确保volatile 读之后的操作不会被编译器重排序到volatile 读之前。
3. 当第一个操作是volatile 写，第二个操作是volatile 读时，不能重排序。

## JVM-JMM-CPU底层全执行流程

![](https://gitee.com/HumorGeeks/img/raw/master/img/202112091039052.jpg)

## 目前操作系统主要结构

![](https://gitee.com/HumorGeeks/img/raw/master/img/202112091422636.png)

## 硬件缓存锁定机制

### 总线锁

> 前端总线(也叫CPU总线)是所有CPU与芯片组连接的主干道，负责CPU与外界所有部件的通信，包括高速缓存、内存、北桥，其控制总线向各个部件发送控制信号、通过地址总线发送地址信号指定其要访问的部件、通过数据总线双向传输

在CPU1要做 i++操作的时候，其在总线上发出一个LOCK#信号，其他处理器就不能操作缓存了该共享变量内存地址的缓存，也就是阻塞了其他CPU，使该处理器可以独享此共享内存

### MESI协议

> 多核CPU的情况下有多个一级缓存，如何保证缓存内部数据的一致,不让系统数据混乱。这里就引出了一个一致性的协议MESI

PS：

**缓存一致性协议基于缓存行，如果超过大小则会升级为总线锁；总线裁决效率远高于总线锁**

**volatile 无法保证原子性**

**缓存一致性协议只能对缓存行有效，二对于寄存器无效**

# 如何解决线程并发安全问题

> 序列化访问临界资源。即在同一时刻，只能有一个线程访问临界资源，也称作同步互斥访问

## 实现方式

### synchronized

> synchronized内置锁是一种对象锁(锁的是对象而非引用)，作用粒度是对象，可以用来实现对临界资源的同步互斥访问，是可
> 重入的；加锁解锁由JVM完成

#### 内部实现

![](https://gitee.com/HumorGeeks/img/raw/master/img/202112091625687.png)

Pthread：系统维护

#### 加锁方式

1. 同步实例方法，锁是当前实例对象
2. 同步类方法，锁是当前类对象
3. 同步代码块，锁是括号里面的对象

PS：sout 为同步方法，多个线程同时调用会影响性能

#### 对象的内存布局

![](https://gitee.com/HumorGeeks/img/raw/master/img/202112091630799.png)

#### 对象头如何记录对象锁

![](https://gitee.com/HumorGeeks/img/raw/master/img/202112091632229.png)

PS：**现在我们虚拟机基本是64位的，而64位的对象头有点浪费空间,JVM默认会开启指针压缩，所以基本上也是按32位的形式记录对象头**
**的**

#### 偏向锁、轻量级锁、重量级锁对应的hashcode放在哪里

偏向锁：调用hashcode后，system.lazyhashcode；会升级成轻量级锁

轻量级锁：lock record（markword）

当解释器执行monitorenter字节码轻度锁住一个对象时，就会在获取锁的线程的栈上显式或者隐式分配一个LockRecord，这个LockRecord存储锁对象markword的拷贝(Displaced Mark Word)

重量级锁：记录在monitor里边

PS：jol-core 用于打印对象内存数据

PS：操作系统的大端和小端模式

PS：IVM 内部将偏向锁延迟4s（默认）启动，JVM启动依赖大量的hashmap，class类，大量同步块，十几个线程；避免无谓的偏向锁-轻量级锁-重量级的升级过程

PS：无锁状态（匿名偏向，可偏向状态，只是没有指定线程ID）

PS：偏向锁调用hashcode后，system.lazyhashcode；会升级成轻量级锁（因为内部没有记录hashcode）

![](https://gitee.com/HumorGeeks/img/raw/master/img/202112091659250.png)

#### 线程逃逸分析

#### 优化

1. 锁的膨胀升级（不可逆）
2. 锁的粗化
3. 锁的消除
4. 自旋锁

### AQS 详解

#### 锁的粗话

### Lock

> 显式锁

